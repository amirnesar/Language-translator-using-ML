{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informed-blend",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "august-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "breathing-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "provincial-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "piano-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "output_texts = []\n",
    "input_characters = set()\n",
    "output_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, output_text, _ = line.split('\\t')\n",
    "    \n",
    "    # We use 'tab' as the 'start sequence' character\n",
    "    # for the targets, and '\\n' as the 'end sequence' character.\n",
    "    \n",
    "    output_text = '\\t' + output_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in output_text:\n",
    "        if char not in output_characters:\n",
    "            output_characters.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attractive-biology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "earlier-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "output_characters = sorted(list(output_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(output_characters)\n",
    "max_encoder_seq_length = max([len(text) for text in input_texts])\n",
    "max_decoder_seq_length = max([len(text) for text in output_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greater-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 20000\n",
      "Number of unique input Tokens: 74\n",
      "Number of unique output Tokens: 101\n",
      "Max sequence length for inputs: 17\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples:\", len(input_texts))\n",
    "print('Number of unique input Tokens:' , num_encoder_tokens)\n",
    "print('Number of unique output Tokens:' , num_decoder_tokens)\n",
    "print('Max sequence length for inputs:' , max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:' , max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hawaiian-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(input_characters)])\n",
    "\n",
    "output_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(output_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comic-bunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({' ': 0,\n",
       "  '!': 1,\n",
       "  '\"': 2,\n",
       "  '$': 3,\n",
       "  '%': 4,\n",
       "  '&': 5,\n",
       "  \"'\": 6,\n",
       "  ',': 7,\n",
       "  '-': 8,\n",
       "  '.': 9,\n",
       "  '0': 10,\n",
       "  '1': 11,\n",
       "  '2': 12,\n",
       "  '3': 13,\n",
       "  '4': 14,\n",
       "  '5': 15,\n",
       "  '6': 16,\n",
       "  '7': 17,\n",
       "  '8': 18,\n",
       "  '9': 19,\n",
       "  ':': 20,\n",
       "  '?': 21,\n",
       "  'A': 22,\n",
       "  'B': 23,\n",
       "  'C': 24,\n",
       "  'D': 25,\n",
       "  'E': 26,\n",
       "  'F': 27,\n",
       "  'G': 28,\n",
       "  'H': 29,\n",
       "  'I': 30,\n",
       "  'J': 31,\n",
       "  'K': 32,\n",
       "  'L': 33,\n",
       "  'M': 34,\n",
       "  'N': 35,\n",
       "  'O': 36,\n",
       "  'P': 37,\n",
       "  'Q': 38,\n",
       "  'R': 39,\n",
       "  'S': 40,\n",
       "  'T': 41,\n",
       "  'U': 42,\n",
       "  'V': 43,\n",
       "  'W': 44,\n",
       "  'Y': 45,\n",
       "  'Z': 46,\n",
       "  'a': 47,\n",
       "  'b': 48,\n",
       "  'c': 49,\n",
       "  'd': 50,\n",
       "  'e': 51,\n",
       "  'f': 52,\n",
       "  'g': 53,\n",
       "  'h': 54,\n",
       "  'i': 55,\n",
       "  'j': 56,\n",
       "  'k': 57,\n",
       "  'l': 58,\n",
       "  'm': 59,\n",
       "  'n': 60,\n",
       "  'o': 61,\n",
       "  'p': 62,\n",
       "  'q': 63,\n",
       "  'r': 64,\n",
       "  's': 65,\n",
       "  't': 66,\n",
       "  'u': 67,\n",
       "  'v': 68,\n",
       "  'w': 69,\n",
       "  'x': 70,\n",
       "  'y': 71,\n",
       "  'z': 72,\n",
       "  'é': 73},\n",
       " {'\\t': 0,\n",
       "  '\\n': 1,\n",
       "  ' ': 2,\n",
       "  '!': 3,\n",
       "  '\"': 4,\n",
       "  '$': 5,\n",
       "  '%': 6,\n",
       "  '&': 7,\n",
       "  \"'\": 8,\n",
       "  '(': 9,\n",
       "  ')': 10,\n",
       "  ',': 11,\n",
       "  '-': 12,\n",
       "  '.': 13,\n",
       "  '0': 14,\n",
       "  '1': 15,\n",
       "  '2': 16,\n",
       "  '3': 17,\n",
       "  '4': 18,\n",
       "  '5': 19,\n",
       "  '6': 20,\n",
       "  '7': 21,\n",
       "  '8': 22,\n",
       "  '9': 23,\n",
       "  ':': 24,\n",
       "  '?': 25,\n",
       "  'A': 26,\n",
       "  'B': 27,\n",
       "  'C': 28,\n",
       "  'D': 29,\n",
       "  'E': 30,\n",
       "  'F': 31,\n",
       "  'G': 32,\n",
       "  'H': 33,\n",
       "  'I': 34,\n",
       "  'J': 35,\n",
       "  'K': 36,\n",
       "  'L': 37,\n",
       "  'M': 38,\n",
       "  'N': 39,\n",
       "  'O': 40,\n",
       "  'P': 41,\n",
       "  'Q': 42,\n",
       "  'R': 43,\n",
       "  'S': 44,\n",
       "  'T': 45,\n",
       "  'U': 46,\n",
       "  'V': 47,\n",
       "  'Y': 48,\n",
       "  'Z': 49,\n",
       "  'a': 50,\n",
       "  'b': 51,\n",
       "  'c': 52,\n",
       "  'd': 53,\n",
       "  'e': 54,\n",
       "  'f': 55,\n",
       "  'g': 56,\n",
       "  'h': 57,\n",
       "  'i': 58,\n",
       "  'j': 59,\n",
       "  'k': 60,\n",
       "  'l': 61,\n",
       "  'm': 62,\n",
       "  'n': 63,\n",
       "  'o': 64,\n",
       "  'p': 65,\n",
       "  'q': 66,\n",
       "  'r': 67,\n",
       "  's': 68,\n",
       "  't': 69,\n",
       "  'u': 70,\n",
       "  'v': 71,\n",
       "  'w': 72,\n",
       "  'x': 73,\n",
       "  'y': 74,\n",
       "  'z': 75,\n",
       "  '\\xa0': 76,\n",
       "  '«': 77,\n",
       "  '»': 78,\n",
       "  'À': 79,\n",
       "  'Ç': 80,\n",
       "  'É': 81,\n",
       "  'Ê': 82,\n",
       "  'Ô': 83,\n",
       "  'à': 84,\n",
       "  'â': 85,\n",
       "  'ç': 86,\n",
       "  'è': 87,\n",
       "  'é': 88,\n",
       "  'ê': 89,\n",
       "  'ë': 90,\n",
       "  'î': 91,\n",
       "  'ï': 92,\n",
       "  'ô': 93,\n",
       "  'ù': 94,\n",
       "  'û': 95,\n",
       "  'œ': 96,\n",
       "  '\\u2009': 97,\n",
       "  '‘': 98,\n",
       "  '’': 99,\n",
       "  '\\u202f': 100})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index , output_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deluxe-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype = 'float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length , num_decoder_tokens), dtype = 'float32')\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length,num_decoder_tokens) , dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sought-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,output_text) in enumerate(zip(input_texts,output_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]] = 1.\n",
    "    encoder_input_data[i,t+1:,input_token_index[' ']] = 1.\n",
    "    \n",
    "    for t, char in enumerate(output_text):\n",
    "        # decoder_output_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i,t,output_token_index[char]] = 1.\n",
    "        if t>0:\n",
    "            # decoder_output_data will be ahead by one timestep\n",
    "            # and will not include the start character\n",
    "            decoder_output_data[i,t-1,output_token_index[char]] = 1.\n",
    "    decoder_input_data[i,t+1:, output_token_index[' ']] = 1.\n",
    "    decoder_output_data[i , t: , output_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sophisticated-singapore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 74)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "backed-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an input sequence and process it:\n",
    "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state= True)\n",
    "encoder_outputs,state_h,state_c = encoder(encoder_inputs)\n",
    "# we discard 'encoder_outputs' and only keep the states.\n",
    "encoder_states = [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "outdoor-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the decoder , using encoder_states as initial states:\n",
    "decoder_inputs= Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim,return_sequences = True, return_state = True)\n",
    "decoder_outputs, _ ,_ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens,activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "involved-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 119s 441ms/step - loss: 1.0676 - accuracy: 0.7334 - val_loss: 0.9594 - val_accuracy: 0.7389\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.6946 - accuracy: 0.8029 - val_loss: 0.7539 - val_accuracy: 0.7767\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 105s 418ms/step - loss: 0.5753 - accuracy: 0.8310 - val_loss: 0.6696 - val_accuracy: 0.8028\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.5146 - accuracy: 0.8481 - val_loss: 0.6143 - val_accuracy: 0.8196\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.4724 - accuracy: 0.8595 - val_loss: 0.5870 - val_accuracy: 0.8249\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.4406 - accuracy: 0.8683 - val_loss: 0.5510 - val_accuracy: 0.8349\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.4142 - accuracy: 0.8758 - val_loss: 0.5300 - val_accuracy: 0.8420\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.3918 - accuracy: 0.8824 - val_loss: 0.5128 - val_accuracy: 0.8465\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 101s 405ms/step - loss: 0.3717 - accuracy: 0.8884 - val_loss: 0.4956 - val_accuracy: 0.8524\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.3540 - accuracy: 0.8934 - val_loss: 0.4841 - val_accuracy: 0.8556\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 101s 402ms/step - loss: 0.3384 - accuracy: 0.8979 - val_loss: 0.4754 - val_accuracy: 0.8587\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 105s 420ms/step - loss: 0.3239 - accuracy: 0.9024 - val_loss: 0.4667 - val_accuracy: 0.8613\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 106s 422ms/step - loss: 0.3107 - accuracy: 0.9061 - val_loss: 0.4638 - val_accuracy: 0.8636\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 103s 413ms/step - loss: 0.2989 - accuracy: 0.9097 - val_loss: 0.4551 - val_accuracy: 0.8665\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 105s 420ms/step - loss: 0.2877 - accuracy: 0.9129 - val_loss: 0.4546 - val_accuracy: 0.8672\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 105s 419ms/step - loss: 0.2776 - accuracy: 0.9159 - val_loss: 0.4500 - val_accuracy: 0.8676\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.2681 - accuracy: 0.9186 - val_loss: 0.4497 - val_accuracy: 0.8692\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.2590 - accuracy: 0.9212 - val_loss: 0.4492 - val_accuracy: 0.8702\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.2508 - accuracy: 0.9237 - val_loss: 0.4465 - val_accuracy: 0.8718\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 97s 390ms/step - loss: 0.2429 - accuracy: 0.9259 - val_loss: 0.4483 - val_accuracy: 0.8715\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2355 - accuracy: 0.9281 - val_loss: 0.4471 - val_accuracy: 0.8727\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 105s 420ms/step - loss: 0.2282 - accuracy: 0.9304 - val_loss: 0.4530 - val_accuracy: 0.8718\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 104s 417ms/step - loss: 0.2216 - accuracy: 0.9321 - val_loss: 0.4528 - val_accuracy: 0.8729\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 104s 415ms/step - loss: 0.2153 - accuracy: 0.9341 - val_loss: 0.4556 - val_accuracy: 0.8728\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 104s 416ms/step - loss: 0.2089 - accuracy: 0.9359 - val_loss: 0.4556 - val_accuracy: 0.8733\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 105s 421ms/step - loss: 0.2031 - accuracy: 0.9376 - val_loss: 0.4624 - val_accuracy: 0.8730\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 103s 411ms/step - loss: 0.1972 - accuracy: 0.9394 - val_loss: 0.4664 - val_accuracy: 0.8731\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 98s 390ms/step - loss: 0.1921 - accuracy: 0.9407 - val_loss: 0.4678 - val_accuracy: 0.8731\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.1867 - accuracy: 0.9425 - val_loss: 0.4687 - val_accuracy: 0.8738\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.1820 - accuracy: 0.9438 - val_loss: 0.4745 - val_accuracy: 0.8726\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.1772 - accuracy: 0.9451 - val_loss: 0.4781 - val_accuracy: 0.8734\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.1725 - accuracy: 0.9467 - val_loss: 0.4830 - val_accuracy: 0.8723\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 0.1682 - accuracy: 0.9478 - val_loss: 0.4882 - val_accuracy: 0.8731\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.1640 - accuracy: 0.9491 - val_loss: 0.4926 - val_accuracy: 0.8720\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.1598 - accuracy: 0.9504 - val_loss: 0.4957 - val_accuracy: 0.8729\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 97s 390ms/step - loss: 0.1559 - accuracy: 0.9516 - val_loss: 0.4993 - val_accuracy: 0.8724\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 98s 390ms/step - loss: 0.1525 - accuracy: 0.9526 - val_loss: 0.5051 - val_accuracy: 0.8730\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.1487 - accuracy: 0.9536 - val_loss: 0.5092 - val_accuracy: 0.8727\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 105s 419ms/step - loss: 0.1452 - accuracy: 0.9550 - val_loss: 0.5133 - val_accuracy: 0.8723\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 105s 419ms/step - loss: 0.1421 - accuracy: 0.9556 - val_loss: 0.5177 - val_accuracy: 0.8723\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 105s 422ms/step - loss: 0.1385 - accuracy: 0.9568 - val_loss: 0.5239 - val_accuracy: 0.8722\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 105s 422ms/step - loss: 0.1356 - accuracy: 0.9576 - val_loss: 0.5277 - val_accuracy: 0.8715\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 102s 408ms/step - loss: 0.1322 - accuracy: 0.9587 - val_loss: 0.5309 - val_accuracy: 0.8715\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 107s 426ms/step - loss: 0.1295 - accuracy: 0.9594 - val_loss: 0.5350 - val_accuracy: 0.8719\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 105s 420ms/step - loss: 0.1269 - accuracy: 0.9604 - val_loss: 0.5420 - val_accuracy: 0.8714\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 105s 420ms/step - loss: 0.1239 - accuracy: 0.9613 - val_loss: 0.5494 - val_accuracy: 0.8712\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 103s 412ms/step - loss: 0.1214 - accuracy: 0.9619 - val_loss: 0.5553 - val_accuracy: 0.8711\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 105s 419ms/step - loss: 0.1188 - accuracy: 0.9627 - val_loss: 0.5568 - val_accuracy: 0.8708\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 105s 420ms/step - loss: 0.1162 - accuracy: 0.9635 - val_loss: 0.5649 - val_accuracy: 0.8711\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.1136 - accuracy: 0.9641 - val_loss: 0.5704 - val_accuracy: 0.8709\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.1116 - accuracy: 0.9646 - val_loss: 0.5733 - val_accuracy: 0.8703\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.1093 - accuracy: 0.9655 - val_loss: 0.5805 - val_accuracy: 0.8708\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.1074 - accuracy: 0.9659 - val_loss: 0.5872 - val_accuracy: 0.8701\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.1052 - accuracy: 0.9666 - val_loss: 0.5909 - val_accuracy: 0.8700\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.1031 - accuracy: 0.9672 - val_loss: 0.5919 - val_accuracy: 0.8713\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 98s 391ms/step - loss: 0.1013 - accuracy: 0.9677 - val_loss: 0.5982 - val_accuracy: 0.8697\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.0996 - accuracy: 0.9682 - val_loss: 0.6038 - val_accuracy: 0.8701\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0977 - accuracy: 0.9689 - val_loss: 0.6076 - val_accuracy: 0.8694\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.0959 - accuracy: 0.9693 - val_loss: 0.6155 - val_accuracy: 0.8699\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.0939 - accuracy: 0.9699 - val_loss: 0.6259 - val_accuracy: 0.8685\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.0923 - accuracy: 0.9704 - val_loss: 0.6284 - val_accuracy: 0.8693\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.0904 - accuracy: 0.9710 - val_loss: 0.6316 - val_accuracy: 0.8691\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 12033s 48s/step - loss: 0.0891 - accuracy: 0.9714 - val_loss: 0.6390 - val_accuracy: 0.8688\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 106s 425ms/step - loss: 0.0878 - accuracy: 0.9715 - val_loss: 0.6374 - val_accuracy: 0.8689\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.0859 - accuracy: 0.9724 - val_loss: 0.6463 - val_accuracy: 0.8683\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 103s 412ms/step - loss: 0.0847 - accuracy: 0.9727 - val_loss: 0.6484 - val_accuracy: 0.8684\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.0834 - accuracy: 0.9731 - val_loss: 0.6559 - val_accuracy: 0.8680\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.0818 - accuracy: 0.9734 - val_loss: 0.6605 - val_accuracy: 0.8683\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.0805 - accuracy: 0.9739 - val_loss: 0.6645 - val_accuracy: 0.8680\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.0793 - accuracy: 0.9742 - val_loss: 0.6713 - val_accuracy: 0.8681\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0781 - accuracy: 0.9745 - val_loss: 0.6736 - val_accuracy: 0.8684\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0767 - accuracy: 0.9750 - val_loss: 0.6732 - val_accuracy: 0.8684\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 2444s 10s/step - loss: 0.0754 - accuracy: 0.9754 - val_loss: 0.6806 - val_accuracy: 0.8686\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.0744 - accuracy: 0.9755 - val_loss: 0.6863 - val_accuracy: 0.8675\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 103s 413ms/step - loss: 0.0731 - accuracy: 0.9761 - val_loss: 0.6895 - val_accuracy: 0.8681\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 104s 414ms/step - loss: 0.0722 - accuracy: 0.9763 - val_loss: 0.6905 - val_accuracy: 0.8675\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 106s 421ms/step - loss: 0.0711 - accuracy: 0.9765 - val_loss: 0.6944 - val_accuracy: 0.8685\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 105s 418ms/step - loss: 0.0700 - accuracy: 0.9768 - val_loss: 0.7025 - val_accuracy: 0.8679\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 102s 407ms/step - loss: 0.0689 - accuracy: 0.9772 - val_loss: 0.7090 - val_accuracy: 0.8668\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.0682 - accuracy: 0.9774 - val_loss: 0.7068 - val_accuracy: 0.8680\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.0674 - accuracy: 0.9777 - val_loss: 0.7132 - val_accuracy: 0.8672\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.0663 - accuracy: 0.9780 - val_loss: 0.7158 - val_accuracy: 0.8678\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0656 - accuracy: 0.9783 - val_loss: 0.7231 - val_accuracy: 0.8668\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0645 - accuracy: 0.9784 - val_loss: 0.7261 - val_accuracy: 0.8670\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.0635 - accuracy: 0.9789 - val_loss: 0.7328 - val_accuracy: 0.8665\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.0627 - accuracy: 0.9790 - val_loss: 0.7376 - val_accuracy: 0.8667\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.0621 - accuracy: 0.9793 - val_loss: 0.7422 - val_accuracy: 0.8666\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.0612 - accuracy: 0.9794 - val_loss: 0.7455 - val_accuracy: 0.8661\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0605 - accuracy: 0.9797 - val_loss: 0.7446 - val_accuracy: 0.8671\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.0598 - accuracy: 0.9798 - val_loss: 0.7483 - val_accuracy: 0.8667\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0590 - accuracy: 0.9800 - val_loss: 0.7560 - val_accuracy: 0.8663\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 1010s 4s/step - loss: 0.0582 - accuracy: 0.9804 - val_loss: 0.7571 - val_accuracy: 0.8667\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.0574 - accuracy: 0.9806 - val_loss: 0.7632 - val_accuracy: 0.8662\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.0565 - accuracy: 0.9808 - val_loss: 0.7698 - val_accuracy: 0.8655\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 0.0561 - accuracy: 0.9809 - val_loss: 0.7671 - val_accuracy: 0.8665\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.0558 - accuracy: 0.9810 - val_loss: 0.7731 - val_accuracy: 0.8660\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.7788 - val_accuracy: 0.8660\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.0544 - accuracy: 0.9814 - val_loss: 0.7824 - val_accuracy: 0.8660\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.0539 - accuracy: 0.9815 - val_loss: 0.7861 - val_accuracy: 0.8659\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.0533 - accuracy: 0.9817 - val_loss: 0.7906 - val_accuracy: 0.8649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21ed86538b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# 'encoder_input_data' & 'decoder_input_data' into 'decoder_output_data'\n",
    "model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Run training:\n",
    "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.fit([encoder_input_data,decoder_input_data], decoder_output_data,\n",
    "         batch_size=batch_size,\n",
    "         epochs = epochs,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pointed-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Cours !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define sampling models:\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape = (latent_dim,))\n",
    "decoder_state_input_c = Input(shape = (latent_dim,))\n",
    "decoder_input_states = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "decoder_outputs,state_h,state_c = decoder_lstm(decoder_inputs, initial_state=decoder_input_states)\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs]+decoder_input_states,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i,char) for char,i in input_token_index.items())\n",
    "reverse_output_char_index = dict(\n",
    "    (i,char) for char,i in output_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value= encoder_model.predict(input_seq)\n",
    "    output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "    output_seq[0,0,output_token_index['\\t']] = 1\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentences = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c = decoder_model.predict(\n",
    "            [output_seq]+ states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1, :])\n",
    "        sampled_char = reverse_output_char_index[sampled_token_index]\n",
    "        decoded_sentences += sampled_char\n",
    "\n",
    "        if(sampled_char == '\\n' or len(decoded_sentences) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        \n",
    "        #update the target sequence (of length 1):\n",
    "        output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        output_seq[0,0,sampled_token_index] = 1\n",
    "        \n",
    "        states_value = [h,c]\n",
    "    return decoded_sentences\n",
    "\n",
    "for seq_index in range(20):\n",
    "    # take one sequence for trying out decoding:\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentences = decode_sequence(input_seq)\n",
    "    print('ProjectGurukul Project: English to French Translation ')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:' , decoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
